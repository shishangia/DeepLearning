{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1ViS88Gn48UJPHGEmpT5dzKXevsZDGkxm","authorship_tag":"ABX9TyOGnqj+lnJyXRvajv8u3hSI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"80spaqFab7al"},"outputs":[],"source":["# Import statements\n","import numpy as np\n","import h5py\n","import pickle\n","import re\n","import scipy.io\n","import warnings\n","from copy import deepcopy\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras import backend as K\n","from keras.layers.core import Dense, Activation\n","from keras.layers import Embedding, LSTM\n","from keras.layers.convolutional import Convolution1D, MaxPooling1D\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import np_utils\n","from keras.models import model_from_json\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score"]},{"cell_type":"code","source":["# Normalizer\n","VOWELS = ['a', 'e', 'i', 'o', 'u']\n","\n","def removeRepeat(string):\n","    return re.sub(r'(.)\\1+', r'\\1\\1', string)     \n","\n","def removeVovels(string):\n","    return ''.join([l for l in string.lower() if l not in VOWELS])\n","\n","def token(sentence, remove_vowels=False, remove_repeat=False, minchars=2):\n","    tokens = []\n","    for t in re.findall(\"[a-zA-Z]+\",sentence.lower()):\n","\n","        if len(t)>=minchars:\n","            if remove_vowels:\n","                t=removeVovels(t)\n","            if remove_repeat:\n","                t=removeRepeat(t)\n","            tokens.append(t)\n","    return tokens"],"metadata":{"id":"8Afz9QgPcTS0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filenames\n","Masterdir = '/content/drive/MyDrive/548/Sentimental Analysis/'\n","Datadir = 'Data/'\n","Modeldir = 'Models/'\n","Featuredir = 'Features/'\n","inputdatasetfilename = 'IIITH_Codemixed.txt'\n","experiment_details = 'lstm128_subword'\n","filename = 'match.txt'"],"metadata":{"id":"fRtn7LLac5CQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data I/O formatting\n","SEPERATOR = '\\t'\n","DATA_COLUMN = 1\n","LABEL_COLUMN = 3\n","LABELS = ['0','1','2'] # 0 -> Negative, 1-> Neutral, 2-> Positive\n","mapping_char2num = {}\n","mapping_num2char = {}\n","MAXLEN = 200"],"metadata":{"id":"YDWMRMmOdQoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LSTM Model Parameters\n","# Embedding\n","MAX_FEATURES = 0\n","embedding_size = 128\n","# Convolution\n","filter_length = 3\n","nb_filter = 128\n","pool_length = 3\n","# LSTM\n","lstm_output_size = 128\n","# Training\n","batch_size = 128\n","number_of_epochs = 50\n","numclasses = 3\n","test_size = 0.2"],"metadata":{"id":"GZ1tDIhHdTdn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Purpose -> Data I/O\n","# Input   -> Data file containing sentences and labels along with the global variables\n","# Output  -> Sentences cleaned up in list of lists format along with the labels as a numpy array\n","def parse(Masterdir,filename,seperator,datacol,labelcol,labels):\n","\t# Reads the files and splits data into individual lines\n","\tf=open(Masterdir+Datadir+filename,'r', encoding='utf-8')\n","\tlines = f.read().lower()\n","\tlines = lines.lower().split('\\n')[:-1]\n","\n","\tX_train = []\n","\tY_train = []\n","\t\n","\t# Processes individual lines\n","\tfor line in lines:\n","\t\t# Seperator for the current dataset. Currently '\\t'. \n","\t\tline = line.split(seperator)\n","\t\t# Token is the function which implements basic preprocessing as mentioned in our paper\n","\t\ttokenized_lines = token(line[datacol])\n","\t\t\n","\t\t# Creates character lists\n","\t\tchar_list = []\n","\t\tfor words in tokenized_lines:\n","\t\t\tfor char in words:\n","\t\t\t\tchar_list.append(char)\n","\t\t\tchar_list.append(' ')\n","\t\tX_train.append(char_list)\n","\t\t\n","\t\t# Appends labels\n","\t\tif line[labelcol] == labels[0]:\n","\t\t\tY_train.append(0)\n","\t\tif line[labelcol] == labels[1]:\n","\t\t\tY_train.append(1)\n","\t\tif line[labelcol] == labels[2]:\n","\t\t\tY_train.append(2)\n","\t\n","\t# Converts Y_train to a numpy array\t\n","\tY_train = np.asarray(Y_train)\n","\tassert(len(X_train) == Y_train.shape[0])\n","\n","\treturn [X_train,Y_train]"],"metadata":{"id":"wtbCkwkVdcF4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Purpose -> Convert characters to integers, a unique value for every character\n","# Input   -> Training data (In list of lists format) along with global variables\n","# Output  -> Converted training data along with global variables\n","def convert_char2num(mapping_n2c,mapping_c2n,trainwords,maxlen):\n","\tallchars = []\n","\terrors = 0\n","\n","\t# Creates a list of all characters present in the dataset\n","\tfor line in trainwords:\n","\t\ttry:\n","\t\t\tallchars = set(allchars+line)\n","\t\t\tallchars = list(allchars)\n","\t\texcept:\n","\t\t\terrors += 1\n","\n","\t# Creates character dictionaries for the characters\n","\tcharno = 0\n","\tfor char in allchars:\n","\t\tmapping_char2num[char] = charno\n","\t\tmapping_num2char[charno] = char\n","\t\tcharno += 1\n","\n","  # Checks\n","\tassert(len(allchars)==charno)\n","\n","\t# Converts the data from characters to numbers using dictionaries \n","\tX_train = []\n","\tfor line in trainwords:\n","\t\tchar_list=[]\n","\t\tfor letter in line:\n","\t\t\tchar_list.append(mapping_char2num[letter])\n","\t\tX_train.append(char_list)\n","\tprint(mapping_char2num)\n","\tprint(mapping_num2char)\n"," \n","\t# Pads the X_train to get a uniform vector\n","\tX_train = pad_sequences(X_train[:], maxlen=maxlen)\n","\treturn [X_train,mapping_num2char,mapping_char2num,charno]"],"metadata":{"id":"lrvtL2V_d7s6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Purpose -> Define and train the proposed LSTM network\n","# Input   -> Data, Labels and model hyperparameters\n","# Output  -> Trained LSTM network\n","def RNN(X_train,y_train,args):\n","\t# Sets the model hyperparameters\n","\t# Embedding hyperparameters\n","\tmax_features = args[0]\n","\tmaxlen = args[1]\n","\tembedding_size = args[2]\n","\n","\t# Convolution hyperparameters\n","\tfilter_length = args[3]\n","\tnb_filter = args[4]\n","\tpool_length = args[5]\n","\n","\t# LSTM hyperparameters\n","\tlstm_output_size = args[6]\n","\n","\t# Training hyperparameters\n","\tbatch_size = args[7]\n","\tnb_epoch = args[8]\n","\tnumclasses = args[9]\n","\ttest_size = args[10] \n","\n","\t# Format conversion for y_train for compatibility with Keras\n","\ty_train = np_utils.to_categorical(y_train, numclasses) \n"," \n","\t# Train & Validation data splitting\n","\tX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=test_size, random_state=42)\n","\t\n","\t# Build the sequential model\n","\t# Model Architecture is:\n","\t# Input -> Embedding -> Conv1D+Maxpool1D -> LSTM -> LSTM -> MLP -> Softmax\n","\tprint('Build model...')\n","\tmodel = Sequential()\n","\tmodel.add(Embedding(max_features, embedding_size, input_length=maxlen))\n","\tmodel.add(Convolution1D(filters=nb_filter,\n","\t\t\t\t\t\t\tkernel_size=filter_length,\n","\t\t\t\t\t\t\tpadding='valid',\n","\t\t\t\t\t\t\tactivation='relu',\n","\t\t\t\t\t\t\tstrides=1))\n","\tmodel.add(MaxPooling1D(pool_size=pool_length))\n","\tmodel.add(LSTM(lstm_output_size, dropout=0.2, return_sequences=True))\n","\tmodel.add(LSTM(lstm_output_size, dropout=0.2, return_sequences=False))\n","\tmodel.add(Dense(numclasses))\n","\tmodel.add(Activation('softmax'))\n","\n","\t# Optimizer is Adamax along with categorical crossentropy loss\n","\tmodel.compile(loss='categorical_crossentropy',\n","\t\t\t  \toptimizer='adamax',\n","\t\t\t  \tmetrics=['accuracy'])\n","\t\n","\n","\tprint('Train...')\n","\t# Trains model for 50 epochs with shuffling after every epoch for training data and validates on validation data\n","\tmodel.fit(X_train, y_train,\n","\t\t\t  batch_size=batch_size,\n","\t\t\t  shuffle=True,\n","\t\t\t  epochs=nb_epoch,\n","\t\t\t  validation_data=(X_valid, y_valid))\n","\treturn model"],"metadata":{"id":"pg1SSU4oeMne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Purpose -> Saves Keras model files to the given directory\n","# Input   -> Directory and experiment details to be saved and trained model file\n","# Output  -> Nil\n","def save_model(Masterdir,filename,model):\n","\tmodel.save_weights(Masterdir + Modeldir + 'LSTM_' + filename + '_weights.h5')\n","\tjson_string = model.to_json()\n","\tf = open(Masterdir + Modeldir + 'LSTM_' + filename + '_architecture.json','w')\n","\tf.write(json_string)\n","\tf.close()"],"metadata":{"id":"WXmpX5yZejPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Purpose -> Obtains outputs from any layer in Keras\n","# Input   -> Trained model, layer from which output needs to be extracted & files to be given as input\n","# Output  -> Features from that layer \n","def get_activations(model, layer, X_batch):\n","\tget_activations = K.function([model.input], [model.layers[layer].output])\n","\tactivations = get_activations(X_batch)\n","\treturn activations"],"metadata":{"id":"5nTsPdhIeut0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Purpose -> Evaluate any model on the testing data\n","# Input   -> Testing data and labels, trained model and global variables\n","# Output  -> Nil\n","def evaluate_model(X_test,y_test,model,batch_size,numclasses):\n","\t# Convert y_test to one-hot encoding\n","\ty_test = np_utils.to_categorical(y_test, numclasses)\n"," \n","\t# Evaluate the accuracies\n","\tscore, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n","\tprint('Test score:', score)\n","\tprint('Test accuracy:', acc)"],"metadata":{"id":"AaZ8dhjDfMAu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Purpose -> Saves train, test data along with labels and features in the respective directories in the folder\n","# Input   -> Train and test data, labels and features along with the directory and experiment details to be mentioned\n","# Output  -> Nil\n","def save_data(Masterdir,filename,X_train,X_test,y_train,y_test,features_train,features_test):\n","\t\"\"\"\n","\t\"\"\"\n","\th5f = h5py.File(Masterdir + Datadir + 'Xtrain_' + filename + '.h5', 'w')\n","\th5f.create_dataset('dataset', data=X_train)\n","\th5f.close()\n","\n","\th5f = h5py.File(Masterdir + Datadir + 'Xtest_' + filename + '.h5', 'w')\n","\th5f.create_dataset('dataset', data=X_test)\n","\th5f.close()\n","\n","\toutput = open(Masterdir + Datadir + 'Ytrain_' + filename + '.pkl', 'wb')\n","\tpickle.dump([y_train], output)\n","\toutput.close()\n","\n","\toutput = open(Masterdir + Datadir + 'Ytest_' + filename + '.pkl', 'wb')\n","\tpickle.dump([y_test], output)\n","\toutput.close()\n","\n","\th5f = h5py.File(Masterdir + Featuredir + 'features_train_' + filename + '.h5', 'w')\n","\th5f.create_dataset('dataset', data=features_train)\n","\th5f.close()\n","\n","\th5f = h5py.File(Masterdir + Featuredir + 'features_test_' + filename + '.h5', 'w')\n","\th5f.create_dataset('dataset', data=features_test)\n","\th5f.close()"],"metadata":{"id":"KvQiAJNvfWsq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training\n","print('Starting RNN Engine...\\nModel: Char-level LSTM.\\nParsing data files...')\n","out = parse(Masterdir,inputdatasetfilename,SEPERATOR,DATA_COLUMN,LABEL_COLUMN,LABELS)\n","X_train = out[0]\n","y_train = out[1]\n","print('Parsing complete!')\n","\n","print('Creating character dictionaries and format conversion in progess...')\n","out = convert_char2num(mapping_num2char,mapping_char2num,X_train,MAXLEN)\n","mapping_num2char = out[1]\n","mapping_char2num = out[2]\n","MAX_FEATURES = out[3]\n","X_train = np.asarray(out[0])\n","y_train = np.asarray(y_train).flatten()\n","print('Complete!')\n","\n","print('Splitting data into train and test...')\n","X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","print('X_train shape:', X_train.shape)\n","print('X_test shape:', X_test.shape)\n","\n","print('Creating LSTM Network...')\n","model = RNN(deepcopy(X_train),deepcopy(y_train),[MAX_FEATURES, MAXLEN, embedding_size,\\\n","          filter_length, nb_filter, pool_length, lstm_output_size, batch_size, \\\n","          number_of_epochs, numclasses, test_size])\n","\n","print('Evaluating model...')\n","evaluate_model(X_test,deepcopy(y_test),model,batch_size,numclasses)\n","\n","print('Feature extraction pipeline running...')\n","activations = get_activations(model, 4, X_train)\n","features_train = np.asarray(activations)\n","activations = get_activations(model, 4, X_test)\n","features_test = np.asarray(activations)\n","print('Features extracted!')\n","\n","print('Saving experiment...')\n","save_model(Masterdir,experiment_details,model)\n","save_data(Masterdir,experiment_details,X_train,X_test,y_train,y_test,features_train,features_test)\n","print('Saved! Experiment finished!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bt3ABewMgBEE","executionInfo":{"status":"ok","timestamp":1683234204773,"user_tz":420,"elapsed":47135,"user":{"displayName":"Shivam Chandresh Shishangia","userId":"17981651343672952383"}},"outputId":"0c108d1e-cc45-46d6-d7ce-997e7b643460"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting RNN Engine...\n","Model: Char-level LSTM.\n","Parsing data files...\n","Parsing complete!\n","Creating character dictionaries and format conversion in progess...\n","{'o': 0, 'v': 1, 'u': 2, 'z': 3, 'g': 4, 'l': 5, 'h': 6, 's': 7, 'c': 8, 't': 9, 'n': 10, 'e': 11, 'r': 12, 'q': 13, 'w': 14, 'p': 15, 'f': 16, 'm': 17, 'x': 18, 'b': 19, ' ': 20, 'y': 21, 'a': 22, 'd': 23, 'j': 24, 'k': 25, 'i': 26}\n","{0: 'o', 1: 'v', 2: 'u', 3: 'z', 4: 'g', 5: 'l', 6: 'h', 7: 's', 8: 'c', 9: 't', 10: 'n', 11: 'e', 12: 'r', 13: 'q', 14: 'w', 15: 'p', 16: 'f', 17: 'm', 18: 'x', 19: 'b', 20: ' ', 21: 'y', 22: 'a', 23: 'd', 24: 'j', 25: 'k', 26: 'i'}\n","Complete!\n","Splitting data into train and test...\n","X_train shape: (3103, 200)\n","X_test shape: (776, 200)\n","Creating LSTM Network...\n","Build model...\n","Train...\n","Epoch 1/50\n","20/20 [==============================] - 7s 83ms/step - loss: 0.9976 - accuracy: 0.5068 - val_loss: 0.9905 - val_accuracy: 0.4815\n","Epoch 2/50\n","20/20 [==============================] - 0s 20ms/step - loss: 0.9701 - accuracy: 0.5133 - val_loss: 1.0069 - val_accuracy: 0.4815\n","Epoch 3/50\n","20/20 [==============================] - 0s 21ms/step - loss: 0.9631 - accuracy: 0.5133 - val_loss: 0.9892 - val_accuracy: 0.4815\n","Epoch 4/50\n","20/20 [==============================] - 0s 20ms/step - loss: 0.9562 - accuracy: 0.5193 - val_loss: 0.9766 - val_accuracy: 0.4928\n","Epoch 5/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.9439 - accuracy: 0.5399 - val_loss: 0.9741 - val_accuracy: 0.5024\n","Epoch 6/50\n","20/20 [==============================] - 0s 18ms/step - loss: 0.9167 - accuracy: 0.5576 - val_loss: 0.9375 - val_accuracy: 0.5346\n","Epoch 7/50\n","20/20 [==============================] - 0s 21ms/step - loss: 0.8867 - accuracy: 0.5854 - val_loss: 0.9067 - val_accuracy: 0.5717\n","Epoch 8/50\n","20/20 [==============================] - 0s 20ms/step - loss: 0.8624 - accuracy: 0.5955 - val_loss: 0.8967 - val_accuracy: 0.5878\n","Epoch 9/50\n","20/20 [==============================] - 0s 20ms/step - loss: 0.8495 - accuracy: 0.6104 - val_loss: 0.9169 - val_accuracy: 0.5523\n","Epoch 10/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.8494 - accuracy: 0.6044 - val_loss: 0.9018 - val_accuracy: 0.5813\n","Epoch 11/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.8317 - accuracy: 0.6152 - val_loss: 0.8719 - val_accuracy: 0.5990\n","Epoch 12/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.8202 - accuracy: 0.6185 - val_loss: 0.8887 - val_accuracy: 0.5862\n","Epoch 13/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.8156 - accuracy: 0.6273 - val_loss: 0.8636 - val_accuracy: 0.5926\n","Epoch 14/50\n","20/20 [==============================] - 0s 20ms/step - loss: 0.8013 - accuracy: 0.6350 - val_loss: 0.8666 - val_accuracy: 0.5878\n","Epoch 15/50\n","20/20 [==============================] - 0s 21ms/step - loss: 0.7887 - accuracy: 0.6418 - val_loss: 0.8621 - val_accuracy: 0.5926\n","Epoch 16/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.7864 - accuracy: 0.6386 - val_loss: 0.8533 - val_accuracy: 0.5926\n","Epoch 17/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.7805 - accuracy: 0.6491 - val_loss: 0.8578 - val_accuracy: 0.6006\n","Epoch 18/50\n","20/20 [==============================] - 0s 20ms/step - loss: 0.7725 - accuracy: 0.6563 - val_loss: 0.8333 - val_accuracy: 0.6167\n","Epoch 19/50\n","20/20 [==============================] - 0s 18ms/step - loss: 0.7687 - accuracy: 0.6543 - val_loss: 0.8463 - val_accuracy: 0.6006\n","Epoch 20/50\n","20/20 [==============================] - 0s 20ms/step - loss: 0.7589 - accuracy: 0.6612 - val_loss: 0.8428 - val_accuracy: 0.5990\n","Epoch 21/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.7531 - accuracy: 0.6668 - val_loss: 0.8430 - val_accuracy: 0.6023\n","Epoch 22/50\n","20/20 [==============================] - 0s 21ms/step - loss: 0.7497 - accuracy: 0.6684 - val_loss: 0.8629 - val_accuracy: 0.5926\n","Epoch 23/50\n","20/20 [==============================] - 0s 22ms/step - loss: 0.7398 - accuracy: 0.6720 - val_loss: 0.8381 - val_accuracy: 0.6055\n","Epoch 24/50\n","20/20 [==============================] - 0s 22ms/step - loss: 0.7262 - accuracy: 0.6745 - val_loss: 0.8425 - val_accuracy: 0.6055\n","Epoch 25/50\n","20/20 [==============================] - 0s 20ms/step - loss: 0.7197 - accuracy: 0.6934 - val_loss: 0.8363 - val_accuracy: 0.6087\n","Epoch 26/50\n","20/20 [==============================] - 0s 24ms/step - loss: 0.7155 - accuracy: 0.6817 - val_loss: 0.8221 - val_accuracy: 0.6264\n","Epoch 27/50\n","20/20 [==============================] - 0s 22ms/step - loss: 0.6927 - accuracy: 0.7027 - val_loss: 0.8433 - val_accuracy: 0.6151\n","Epoch 28/50\n","20/20 [==============================] - 0s 22ms/step - loss: 0.6958 - accuracy: 0.6954 - val_loss: 0.8380 - val_accuracy: 0.5990\n","Epoch 29/50\n","20/20 [==============================] - 0s 22ms/step - loss: 0.6938 - accuracy: 0.6958 - val_loss: 0.8305 - val_accuracy: 0.6119\n","Epoch 30/50\n","20/20 [==============================] - 0s 22ms/step - loss: 0.6920 - accuracy: 0.6970 - val_loss: 0.8674 - val_accuracy: 0.6200\n","Epoch 31/50\n","20/20 [==============================] - 0s 21ms/step - loss: 0.7039 - accuracy: 0.6962 - val_loss: 0.8107 - val_accuracy: 0.6184\n","Epoch 32/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.6825 - accuracy: 0.7156 - val_loss: 0.8159 - val_accuracy: 0.6119\n","Epoch 33/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.6592 - accuracy: 0.7147 - val_loss: 0.8275 - val_accuracy: 0.6264\n","Epoch 34/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.6658 - accuracy: 0.7099 - val_loss: 0.8280 - val_accuracy: 0.6135\n","Epoch 35/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.6591 - accuracy: 0.7147 - val_loss: 0.8272 - val_accuracy: 0.6216\n","Epoch 36/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.6538 - accuracy: 0.7228 - val_loss: 0.8397 - val_accuracy: 0.6248\n","Epoch 37/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.6396 - accuracy: 0.7297 - val_loss: 0.8779 - val_accuracy: 0.6296\n","Epoch 38/50\n","20/20 [==============================] - 0s 20ms/step - loss: 0.6394 - accuracy: 0.7333 - val_loss: 0.8515 - val_accuracy: 0.6312\n","Epoch 39/50\n","20/20 [==============================] - 0s 21ms/step - loss: 0.6305 - accuracy: 0.7365 - val_loss: 0.8375 - val_accuracy: 0.6264\n","Epoch 40/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.6240 - accuracy: 0.7373 - val_loss: 0.8189 - val_accuracy: 0.6538\n","Epoch 41/50\n","20/20 [==============================] - 0s 21ms/step - loss: 0.6126 - accuracy: 0.7434 - val_loss: 0.8092 - val_accuracy: 0.6522\n","Epoch 42/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.6063 - accuracy: 0.7442 - val_loss: 0.7921 - val_accuracy: 0.6602\n","Epoch 43/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.6036 - accuracy: 0.7446 - val_loss: 0.8579 - val_accuracy: 0.6473\n","Epoch 44/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.6159 - accuracy: 0.7385 - val_loss: 0.8006 - val_accuracy: 0.6522\n","Epoch 45/50\n","20/20 [==============================] - 0s 20ms/step - loss: 0.6012 - accuracy: 0.7470 - val_loss: 0.8006 - val_accuracy: 0.6425\n","Epoch 46/50\n","20/20 [==============================] - 0s 21ms/step - loss: 0.5871 - accuracy: 0.7554 - val_loss: 0.8188 - val_accuracy: 0.6554\n","Epoch 47/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.6035 - accuracy: 0.7397 - val_loss: 0.8855 - val_accuracy: 0.6280\n","Epoch 48/50\n","20/20 [==============================] - 0s 18ms/step - loss: 0.5959 - accuracy: 0.7550 - val_loss: 0.8519 - val_accuracy: 0.6441\n","Epoch 49/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.5621 - accuracy: 0.7538 - val_loss: 0.8287 - val_accuracy: 0.6506\n","Epoch 50/50\n","20/20 [==============================] - 0s 19ms/step - loss: 0.5668 - accuracy: 0.7603 - val_loss: 0.8583 - val_accuracy: 0.6473\n","Evaluating model...\n","7/7 [==============================] - 0s 11ms/step - loss: 0.8068 - accuracy: 0.6778\n","Test score: 0.8067740797996521\n","Test accuracy: 0.6778350472450256\n","Feature extraction pipeline running...\n","Features extracted!\n","Saving experiment...\n","Saved! Experiment finished!\n"]}]},{"cell_type":"code","source":["# Accuracy\n","def accuracy(original, predicted):\n","\tprint(\"F1 score is: \" + str(f1_score(original, predicted, average='macro')))\n","\tscores = confusion_matrix(original, predicted)\n","\tprint(scores)\n","\tprint(np.trace(scores)/float(np.sum(scores)))"],"metadata":{"id":"0dr37ZdSgOK-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Testing\n","h5f = h5py.File(Masterdir + Datadir + 'Xtest_' + experiment_details + '.h5','r')\n","X_test = h5f['dataset'][:]\n","h5f.close()\n","print(X_test.shape)\n","\n","inp = open(Masterdir + Datadir + 'Ytest_' + experiment_details + '.pkl', 'rb')\n","y_test=pickle.load(inp)\n","inp.close()\n","y_test=np.asarray(y_test).flatten()\n","y_test2 = np_utils.to_categorical(y_test, numclasses) \n","print(y_test.shape)\n","f = open(Masterdir + Modeldir + 'LSTM_' + experiment_details + '_architecture.json','r+')\n","json_string = f.read()\n","f.close()\n","model = model_from_json(json_string)\n","\n","model.load_weights(Masterdir + Modeldir + 'LSTM_' + experiment_details + '_weights.h5')\n","model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])\n","\n","score, acc = model.evaluate(X_test, y_test2, batch_size=batch_size)\n","\n","y_pred = model.predict(X_test, batch_size=batch_size)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","accuracy(y_test,y_pred_classes)\n","\n","print('Accuracy is: '+str(acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOkDzNwhhcun","executionInfo":{"status":"ok","timestamp":1683234217983,"user_tz":420,"elapsed":5332,"user":{"displayName":"Shivam Chandresh Shishangia","userId":"17981651343672952383"}},"outputId":"45d0581a-37ac-4738-ffe3-f5e474736a67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(776, 200)\n","(776,)\n","7/7 [==============================] - 1s 12ms/step - loss: 0.8068 - accuracy: 0.6778\n","7/7 [==============================] - 1s 10ms/step\n","F1 score is: 0.610043376378226\n","[[ 41  67   8]\n"," [ 11 353  31]\n"," [  4 129 132]]\n","0.6778350515463918\n","Accuracy is: 0.6778350472450256\n"]}]}]}